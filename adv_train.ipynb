{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adversarial training using FSGM\n",
    "Models MobileNetV2, ResNet18, EfficientNet-B0\n",
    "'''\n",
    "\n",
    "import torch \n",
    "from datasets import load_from_disk\n",
    "\n",
    "DATA_PATH = \"~/Datasets/cv_project/resized_oxford_pets_22\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "# FSGM perturbation size\n",
    "EPSILON = 0.01\n",
    "\n",
    "'''\n",
    "Dataset class imports whole dataset, in any format\n",
    "    torch.utils.data.Dataset\n",
    "    methdo __len__() to discover size, __getitem__() to get instances by key (default key is integer)\n",
    "    main parameters: dataset, batch size, shuffle\n",
    "DataLoader class pulls samples from dataset, where you specify batch size\n",
    "'''\n",
    "\n",
    "# Load datasets \n",
    "\n",
    "ds = load_from_disk(DATA_PATH) # Visualize at https://huggingface.co/datasets/visual-layer/oxford-iiit-pet-vl-enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n",
      "Transforms defined!\n",
      "Applying transforms...\n",
      "Pulling batches...\n",
      "train size: 3638, test size: 3626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "from datasets import ClassLabel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms \n",
    "import numpy as np\n",
    "\n",
    "img_col   = 'image'\n",
    "label_col = 'label_breed'\n",
    "\n",
    "# Build a map from string labels to int\n",
    "labels = ds['train'].unique(label_col)\n",
    "labels.sort()                        \n",
    "label2int = {v: i for i, v in enumerate(labels)}\n",
    "\n",
    "print('Prepare data...')\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "print('Transforms defined!')\n",
    "print('Applying transforms...')\n",
    "\n",
    "# Apply transforms \n",
    "def preprocess(example):\n",
    "    img = np.array(example[img_col], dtype=np.uint8)\n",
    "    example['pixel_values'] = transform(img)\n",
    "    example['labels'] = torch.tensor(label2int[example[label_col]], dtype=torch.long)\n",
    "    return example\n",
    "\n",
    "ds = ds.map(preprocess, remove_columns=[img_col, label_col])\n",
    "ds.set_format(type='torch', columns=['pixel_values', 'labels'])\n",
    "\n",
    "print('Pulling batches...')\n",
    "\n",
    "# Pull batches\n",
    "train_loader = DataLoader(ds['train'], batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "test_loader  = DataLoader(ds['test'],  batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Sanity check\n",
    "print(f\"train size: {len(ds['train'])}, test size: {len(ds['test'])}\")\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['pixel_values'].shape, batch['labels'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device before loading models and optimizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matildedolfato/venvs/cv_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/matildedolfato/venvs/cv_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/matildedolfato/venvs/cv_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matildedolfato/venvs/cv_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load Models \n",
    "\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "\n",
    "num_classes = len(label2int)\n",
    "\n",
    "# MobileNetV2\n",
    "mobilenet = models.mobilenet_v2(weights=True)\n",
    "mobilenet.classifier[1] = nn.Linear(\n",
    "    mobilenet.classifier[1].in_features, num_classes)\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# ResNet18 (standard fc)\n",
    "resnet = models.resnet18(weights=True)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# EfficientNet-B0\n",
    "effnet = models.efficientnet_b0(weights=True)\n",
    "in_feats = effnet.classifier[1].in_features\n",
    "effnet.classifier[1] = nn.Linear(in_feats, num_classes)\n",
    "effnet = effnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial training loop (FSGM) \n",
    "# Reference: https://arxiv.org/pdf/1412.6572\n",
    "\n",
    "'''\n",
    "Adversarial training is about becoming aware of the models' security and robustness\n",
    "The overarching goal of an adversary is to add the least amount of perturbation to the input data to cause the desired misclassification, fooling the model\n",
    "\n",
    "Fast Gradient Sign Attack (FSGM) is a white-box attack, i.e. the attacker has full knowledge and access to the model (architecture, inputs, outputs, and weights)\n",
    "which doesn't care of which label the model will give, as long as it misclassifies\n",
    "Idea: it uses gradients to perturbe inputs and fool the model\n",
    "How it works: at each batch iteration\n",
    " - take an input x and compute gradient of loss wrt x, fixing parameters at current value ('E'ven early in training, neural networks exhibit locally linear behavior — especially in high dimensions.' hence gradient direction is already the \"right\" one)\n",
    " - observe sign of gradient \n",
    " - perturb input in THAT direction (loss will be higher): x' <- x + e*grad\n",
    " - train model half with x and half x'\n",
    " - for a big enough perturbation e, the model will misclassify x' \n",
    " > result: regularized and more robust model!\n",
    "'''\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Normalization stats (same as in transforms)\n",
    "means = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "stds  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
    "\n",
    "# Precompute the valid min/max for each channel in *normalized* space\n",
    "min_val = (0.0 - means) / stds\n",
    "max_val = (1.0 - means) / stds\n",
    "\n",
    "def fgsm_attack(model, loss_f, x, y, epsilon):\n",
    "    \"\"\"\n",
    "    Given a batch (x, y), returns adversarial examples x_adv\n",
    "    via one-step FGSM under L_inf constraint epsilon.\n",
    "    it needs all the model, loss and inputs parameters as it does a backprop step altogether!\n",
    "    \"\"\"\n",
    "    # Make a copy of x that we can take gradients wrt\n",
    "    x_copy = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Fw and Bw pass to compute graident of the loss wrt input x \n",
    "    outputs = model(x_copy) \n",
    "    loss = loss_f(outputs, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perturbe x!\n",
    "    sign_grad = x_copy.grad.sign()\n",
    "    x_adv = x_copy + epsilon * sign_grad\n",
    "    \n",
    "    # if your inputs are normalized, you may want to clamp in normalized space\n",
    "    # here we assume pixel range [0, 1]:\n",
    "    x_adv = torch.max(torch.min(x_adv, max_val), min_val)\n",
    "    \n",
    "    return x_adv.detach()\n",
    "\n",
    "# Train on adversarial examples \n",
    "def train_model_adv(model, loader, epochs=NUM_EPOCHS, epsilon=EPSILON):\n",
    "    model.train()\n",
    "    loss_f = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0.0\n",
    "        for batch in loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "                        \n",
    "            # Compute perturbed inputs!\n",
    "            inputs_adv = fgsm_attack(model, loss_f, inputs, labels, epsilon)\n",
    "\n",
    "            # Train half on perturbed half on clean (concretely averaging losses is the same as combining the dataset)\n",
    "            outputs_clean = model(inputs)\n",
    "            outputs_adv = model(inputs_adv)\n",
    "            loss_clean = loss_f(outputs_clean, labels)\n",
    "            loss_adv = loss_f(outputs_adv, labels)\n",
    "            loss = 0.5 * loss_clean + 0.5 * loss_adv\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(loader.dataset)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Adv Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv. training mobilenet...\n",
      "Epoch 1/10 - Adv Loss: 0.0756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobilenet\u001b[39m\u001b[38;5;124m\"\u001b[39m, mobilenet), \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#(\"resnet\", resnet), \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#(\"effnet\", effnet)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ]:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdv. training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mtrain_model_adv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_adv.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[25], line 81\u001b[0m, in \u001b[0;36mtrain_model_adv\u001b[0;34m(model, loader, epochs, epsilon)\u001b[0m\n\u001b[1;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 81\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     83\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Adv Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adversarially train models\n",
    "\n",
    "import gc\n",
    "\n",
    "for name, model in [(\"mobilenet\", mobilenet), \n",
    "(\"resnet\", resnet), \n",
    "(\"effnet\", effnet)\n",
    "]:\n",
    "    print(f\"Adv. training {name}...\")\n",
    "    train_model_adv(model, train_loader)\n",
    "    torch.save(model.state_dict(), f\"{name}_adv.pt\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Define top-k accuracy\n",
    "def top_k_accuracy(outputs, labels, k=5):\n",
    "    _, topk_preds = outputs.topk(k, dim=1)\n",
    "    correct = topk_preds.eq(labels.view(-1, 1).expand_as(topk_preds))\n",
    "    return correct.any(dim=1).float().mean().item()\n",
    "\n",
    "# Test models\n",
    "def test_model(model, loader, k=5):\n",
    "    model.eval()\n",
    "    correct1 = 0\n",
    "    correct_k_total = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Top-1\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct1 += (preds == labels).sum().item()\n",
    "            # Top-k\n",
    "            batch_k_acc = top_k_accuracy(outputs, labels, k)\n",
    "            correct_k_total += batch_k_acc * labels.size(0)\n",
    "            total += labels.size(0)\n",
    "    print(f\"Top-1 Accuracy: {correct1 / total:.4f}\")\n",
    "    print(f\"Top-{k} Accuracy: {correct_k_total / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing adv. mobilenet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matildedolfato/venvs/cv_venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mobilenet_adv.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(name)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load weights computed with training\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_adv.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/venvs/cv_venv/lib/python3.10/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/venvs/cv_venv/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/venvs/cv_venv/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mobilenet_adv.pt'"
     ]
    }
   ],
   "source": [
    "# Adversarially test models\n",
    "\n",
    "# Re-load models as before, with weights=False to avoid waste because I load my own weights later, by name\n",
    "def get_model(name):\n",
    "    if name == \"mobilenet\":\n",
    "        model = models.mobilenet_v2(weights=False)\n",
    "        model.classifier[1] = nn.Linear(\n",
    "            model.classifier[1].in_features, num_classes)\n",
    "    elif name == \"resnet\":\n",
    "        model = models.resnet18(weights=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif name == \"effnet\":\n",
    "        model = models.efficientnet_b0(weights=False)\n",
    "        in_feats = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_feats, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {name}\")\n",
    "    return model\n",
    "\n",
    "for name in [\"mobilenet\", \n",
    "\"resnet\", \n",
    "\"effnet\"\n",
    "]:\n",
    "    print(f\"\\nTesting adv. {name}...\")\n",
    "\n",
    "    # Save model to device\n",
    "    model = get_model(name).to(device)\n",
    "    # Load weights computed with training\n",
    "    model.load_state_dict(torch.load(f\"{name}_adv.pt\"))\n",
    "    # Test\n",
    "    model.eval()\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "    # Cleanup memory between models\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_venv",
   "language": "python",
   "name": "cv_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
